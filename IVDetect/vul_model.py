import numpy
import numpy as np
import torch
import torch.nn as nn
from torch.nn import GRU, Dropout
from torch.autograd import Variable as Var
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data, InMemoryDataset
from torch_geometric.nn import global_mean_pool
from tqdm import tqdm

class ChildSumTreeLSTM(nn.Module):
    def __init__(self, in_dim, mem_dim, dropout1):
        # in_dim is the input dim and mem_dim is the output dim
        super(ChildSumTreeLSTM, self).__init__()
        self.in_dim = in_dim
        self.mem_dim = mem_dim
        self.ioux = nn.Linear(self.in_dim, 3 * self.mem_dim)
        self.iouh = nn.Linear(self.mem_dim, 3 * self.mem_dim)
        self.fx = nn.Linear(self.in_dim, self.mem_dim)
        self.fh = nn.Linear(self.mem_dim, self.mem_dim)
        self.H = []
        self.drop = nn.Dropout(dropout1)

    def node_forward(self, inputs, child_c, child_h):
        # print("input", inputs.shape)
        inputs = torch.unsqueeze(inputs, 0)
        # print("input unsqueeze",inputs.shape)
        child_h_sum = torch.sum(child_h, dim=0)
        iou = self.ioux(inputs) + self.iouh(child_h_sum)
        i, o, u = torch.split(iou, iou.size(1) // 3, dim=1)
        i, o, u = torch.sigmoid(i), torch.sigmoid(o), torch.tanh(u)
        f = torch.sigmoid(self.fh(child_h) + self.fx(inputs).repeat(len(child_h), 1))
        fc = torch.mul(f, child_c)
        c = torch.mul(i, u) + torch.sum(fc, dim=0)
        h = torch.mul(o, torch.tanh(c))
        self.H.append(h)
        return c, h

    def forward(self, data):
        tree = data[0]
        inputs = data[1]
        # The inputs here are the tree structure built from class Tree and the input is a list of values with the
        # node ids as the key to store the tree values
        _ = [self.forward([tree.children[idx], inputs]) for idx in range(tree.num_children)]

        if tree.num_children == 0:
            # print("jere",type(inputs[0]))
            # print("before crash",inputs)
            child_c = Var(inputs[tree.id].data.new(1, self.mem_dim).fill_(0.))
            child_h = Var(inputs[tree.id].data.new(1, self.mem_dim).fill_(0.))
        else:
            child_c, child_h = zip(*map(lambda x: x.state, tree.children))
            child_c, child_h = torch.cat(child_c, dim=0), torch.cat(child_h, dim=0)
        # print("id",tree.id,"input len",len(inputs))
        tree.state = self.node_forward(inputs[tree.id], child_c, child_h)
        return tree.state


class Vulnerability(torch.nn.Module):
    def __init__(self, h_size, num_node_feature, num_classes, feature_representation_size, drop_out_rate):
        super(Vulnerability, self).__init__()
        self.h_size = h_size
        self.num_node_feature = num_node_feature
        self.num_classes = num_classes
        self.feature_representation_size = feature_representation_size
        self.drop_out_rate = drop_out_rate
        # The 1th feature input (tree)
        self.tree_lstm = ChildSumTreeLSTM(self.feature_representation_size, self.h_size, self.drop_out_rate)
        # The 2th feature input (sequence)
        self.gru_1 = GRU(input_size=self.feature_representation_size, hidden_size=self.h_size, batch_first=True)
        # The 3th feature input (sequence)
        self.gru_2 = GRU(input_size=self.feature_representation_size, hidden_size=self.h_size, batch_first=True)
#         # The 4th feature input (sequence)
#         self.gru_3 = GRU(input_size=self.feature_representation_size, hidden_size=self.h_size, batch_first=True)
#         # The 5th feature input (sequence)
#         self.gru_4 = GRU(input_size=self.feature_representation_size, hidden_size=self.h_size, batch_first=True)
        # This layer is the bi-directional GRU layer
        self.gru_combine = GRU(input_size=self.h_size, hidden_size=self.h_size, bidirectional=True, batch_first=True)
        # This layer is the GCN Model layer
        # h_size : in channels.  self.num_classes: out channels(2)
        self.conv = GCNConv(self.h_size, self.num_classes)
        self.dropout = Dropout(self.drop_out_rate)
        self.pool = nn.AdaptiveAvgPool2d((1,2))
        self.connect = nn.Linear(self.h_size * self.num_node_feature * 2, self.h_size)

    # def forward(self, data, graph_data, tree_data):
    def forward(self,_Data):
        # print("inforward",_Data)
        data = _Data.my_data
        graph_data = _Data
        tree_data = _Data.tree_info
        # Input data format: a list that contains main graph, feature 1, ..., feature 5. The feature 1 is tree
        # structured and features 2-5 are sequences.
        graph_input = graph_data
        feature_1 = tree_data
        feature_2 = data[0]
        feature_3 = data[1]
#         feature_4 = data[2]
#         feature_5 = data[3]
        edge_index = graph_input.edge_index
        feature_vec1 = None
        # for every statement, get its AST subtree and generete
        for i in range(len(feature_1)):
            if i == 0:
                _, feature_vec1 = self.tree_lstm(feature_1[i])
            else:
                _, feature_vec_temp = self.tree_lstm(feature_1[i])
                feature_vec1 = torch.cat((feature_vec1, feature_vec_temp), 0)
        feature_vec1 = torch.reshape(feature_vec1, (-1, 1, self.h_size))
        feature_vec2, _ = self.gru_1(feature_2)
        feature_vec3, _ = self.gru_2(feature_3)
#         feature_vec4, _ = self.gru_3(feature_4)
#         feature_vec5, _ = self.gru_4(feature_5)
#         print("f2",feature_vec2.size())
        feature_input = torch.cat((feature_vec1, feature_vec2[:, -1:, :], feature_vec3[:, -1:, :]), 1)
        # print("f",feature_input.size())
        feature_vec, _ = self.gru_combine(feature_input)
        feature_vec = self.dropout(feature_vec)
        feature_vec = torch.flatten(feature_vec, 1)
        feature_vec = self.connect(feature_vec)
        conv_output = self.conv(feature_vec, edge_index)
        # print("conv",output)
        # output = nn.Softmax(dim=0)(output)
        # print("softmax:",output)
        # return output
        pooled = global_mean_pool(conv_output,torch.tensor(np.zeros(shape=(conv_output.shape[0]),dtype=int)))
        output = nn.Softmax(dim=1)(pooled)
        return output


class Tree(object):
    # Use this structure to create tree data
    def __init__(self):
        self.parent = None
        self.num_children = 0
        self.children = list()
        self.id = None

    def add_child(self, child):
        child.parent = self
        self.num_children += 1
        self.children.append(child)

    def size(self):
        if getattr(self, '_size'):
            return self._size
        count = 1
        for i in range(self.num_children):
            count += self.children[i].size()
        self._size = count
        return self._size

    def depth(self):
        if getattr(self, '_depth'):
            return self._depth
        count = 0
        if self.num_children > 0:
            for i in range(self.num_children):
                child_depth = self.children[i].depth()
                if child_depth > count:
                    count = child_depth
            count += 1
        self._depth = count
        return self._depth


def build_tree(edge_list, value_list, id, store_value):
    root = Tree()
    # modifed here
    root.id = id
    store_value[id] = value_list.get(id)
    # print(edge_list.get(id))
    if edge_list.get(id):
        for child_id in edge_list.get(id):
            new_child, store_value = build_tree(edge_list, value_list, child_id, store_value)
            root.add_child(new_child)
    return root, store_value


def generate_tree_feature(tree_info):
    # USED FOR GENERATING THE NODE FEATURE (TREE-RELATED) this method needs a list as input. The list stores ASTs for
    # each statement in a method. for each AST, it should have two key values:edge_list, node_value_list. And the
    # ASTs are stored with the same order as statement ids for edge_list, it should match the node_value_list for
    # node id. the sturcture use a dic to represent like: for node A, it has {A: [B,C,D]} means B, C, D are child
    # nodes for it. I expect the node has smaller node id is in the higher level. Like root node always has the
    # smallest node ID for node_value_list, it is a list, it should be {A: value, ...}, here the value should be a
    # representation vector built by embedding tools like Glove and word2vec so the tree_info should like [[{A:...},
    # {A:...}],...]
    output_data = []
    for AST in tree_info:
        edge_list = AST[0]
        node_value_list = AST[1]
        # node_value_list: dict of node: node_value
        c_id = min(AST[0].keys())
        # print(AST[0],"\t",c_id)
        c_value = {}
        c_ast, c_value = build_tree(edge_list, node_value_list, c_id, c_value)
        c_value = {k:torch.from_numpy(v).type(torch.FloatTensor) for (k,v) in c_value.items()}
        output_data.append([c_ast, c_value])
    return output_data


def generate_graph_feature(graph_info, statement_value, steps):
    # print(graph_info)
    # for k,v in statement_value.items():
    #     print(k,v.shape)
    # USED FOR GENERATING THE NODE FEATURE (GRAPH-RELATED) This method requires a dic input [graph_info] for
    # edge_list like: for node A, it has {A: [B,C,D]} means B, C, D are the node linked from A. This method requires
    # a dic input [statement_value] for statement embeddings like: for statement A, it has {A: value}. The value is a
    # sequence of vectors. Each one represent a token in the statement
    output_list = []
    for id in sorted(graph_info.keys()):
        output = []
        if steps == 1:
            for key in sorted(graph_info.keys()):
                if id in graph_info[key]:
                    output.append(statement_value[key])
            for out in graph_info[id]:
                output.append(statement_value[out])
        if steps == 2:
            for key in sorted(graph_info.keys()):
                if id in graph_info[key]:
                    for key_ in sorted(graph_info.keys()):
                        if key in graph_info[key_]:
                            output.append(statement_value[key_])
                    output.append(statement_value[key])
            for out in graph_info[id]:
                for out_ in graph_info[out]:
                    output.append(statement_value[out_])
                output.append(statement_value[out])
        # add to tensor
        #output = [torch.from_numpy(item).float() for item in output]
        # output = [x for x in output if x != []]
        # print(output)
        if output:
            # for o in output:
            #     print(o.shape)
            output = np.concatenate(output)
            output = [torch.from_numpy(item).float() for item in output]
            output = torch.stack(output)
            output_list.append(output)
        else:
            output_list.append(torch.stack([torch.zeros(100)]))
    # print(len(output_list))
    return output_list


def generate_big_graph(edge_list):
    # This one is used to generate the overall PDG for a method
    in_ = []
    out_ = []
    for key in edge_list.keys():
        for value in edge_list[key]:
            in_.append(key)
            out_.append(value)
    output = torch.tensor([in_, out_])
    graph_data = Data(edge_list=torch.tensor(output))
    return graph_data


def train(epochs, trainLoader, model, device, learning_rate):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    # model.to(device)
    model.train()
    for e in range(epochs):
        for _data in tqdm(trainLoader):
            # print("each batch", _data)
            # _data = _data.to(device)
            out = model(_data)
            loss = criterion(out, _data.y)
            # print(loss)
            optimizer.zero_grad()  # if don't call zero_grad, the grad of each batch will be accumulated
            loss.backward()
            optimizer.step()
            # if i % 20 == 0:
            #     print('epoch: {}, batch: {}, loss: {}'.format(e + 1, i + 1, loss.data))
            print('epoch: {},  loss: {}'.format(e + 1, loss.data))
    torch.save(model, 'Put_you_model_name.pth')
