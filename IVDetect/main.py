import os
import pickle
import random
from time import sleep

import torch
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from torch import nn

import vul_model
import torch.nn.utils.rnn as rnn_utils
from tqdm import tqdm

from preprocess import preprocess


def start_training(dataset):
    random.shuffle(dataset)

    for i, data in enumerate(dataset):  # todo: change to pack&unpack, or use RNN layer in utils.improve.py
        for index, _my_datas in enumerate(data.my_data):
            data.my_data[index] = rnn_utils.pad_sequence(_my_datas, batch_first=True, padding_value=0)

    # manual dataset
    train_dataset = dataset[:2000]  # todo: build a dataloader
    test_dataset = dataset[2000:2200]
    print(len(train_dataset))
    print(len(test_dataset))
    model = vul_model.Vulnerability(h_size=64, num_node_feature=3, num_classes=2, feature_representation_size=100,
                                    drop_out_rate=0.5)
    print("model", model)
    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    device = 'cpu'
    train(epochs=10, trainLoader=train_dataset, testLoader=test_dataset, model=model, learning_rate=0.0001)


def evaluate_metrics(model, test_loader):
    model.eval()
    with torch.no_grad():
        all_predictions, all_targets = [], []
        for data in tqdm(test_loader):
            out = model(data)
            pred = out.argmax(dim=1)
            all_predictions.extend(pred.numpy().tolist())
            all_targets.append(data.y)
        print("all tar\t", all_targets)
        print("all pre\t", all_predictions)
        print("acc: ", accuracy_score(all_targets, all_predictions) * 100,
              "precision:", precision_score(all_targets, all_predictions) * 100,
              "recall:", recall_score(all_targets, all_predictions) * 100,
              "f1:", f1_score(all_targets, all_predictions) * 100)


def train(epochs, trainLoader, testLoader, model, learning_rate):
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    model.train()
    try:
        for e in range(epochs):
            for index, _data in enumerate(tqdm(trainLoader, leave=False)):
                out = model(_data)
                _data.y = torch.tensor([_data.y])
                loss = criterion(out, _data.y)
                optimizer.zero_grad()  # if don't call zero_grad, the grad of each batch will be accumulated
                loss.backward()
                optimizer.step()
                sleep(0.05)
                if index % 20 == 0:
                    print('epoch: {}, batch: {}, loss: {}'.format(e + 1, index + 1, loss.data))
            evaluate_metrics(model=model, test_loader=testLoader)
    # torch.save(model, 'Put_you_model_name.pth') # todo, finish model saving part
    except KeyboardInterrupt:
        evaluate_metrics(model=model, test_loader=testLoader)


if __name__ == '__main__':

    input_graph_file = "data/graph_data/devign_sample_output.json"
    processed_full_data_path = "data/input_data/processed_full.bin"

    if os.path.exists(processed_full_data_path):
        print(f'reading processed data from {processed_full_data_path}')
        dataset = pickle.load(open(processed_full_data_path, 'rb'))
    else:
        print(f'processed data not found, generate new one')
        dataset = preprocess(input_graph_file)
        print(f'data processed, now save to {processed_full_data_path}')
        file = open(processed_full_data_path, 'wb')
        pickle.dump(dataset, file)
        file.close()

    start_training(dataset)
