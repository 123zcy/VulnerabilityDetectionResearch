import torch
from torch import nn


class ImprovedRnn(nn.Module):
    def __init__(self, module, *args, **kwargs):
        assert module in (nn.RNN, nn.LSTM, nn.GRU)
        super().__init__()
        self.module = module(*args, **kwargs)

    def forward(self, input, lengths):  # input shape(batch_size, seq_len, input_size)
        if not hasattr(self, '_flattened'):
            self.module.flatten_parameters()
            setattr(self, '_flattened', True)
        max_len = input.shape[1]
        # enforce_sorted=False则自动按lengths排序，并且返回值package.unsorted_indices可用于恢复原顺序
        package = nn.utils.rnn.pack_padded_sequence(input, lengths.cpu(), batch_first=self.module.batch_first,
                                                    enforce_sorted=False)
        result, hidden = self.module(package)
        # total_length参数一般不需要，因为lengths列表中一般含最大值。但分布式训练时是将一个batch切分了，故一定要有！
        result, lens = nn.utils.rnn.pad_packed_sequence(result, batch_first=self.module.batch_first,
                                                        total_length=max_len)
        return result[package.unsorted_indices], hidden  # output shape(batch_size, seq_len, rnn_hidden_size)


class TestNet(nn.Module):

    def __init__(self, word_emb, gru_in, gru_out):
        super().__init__()
        self.encode = nn.Embedding.from_pretrained(torch.Tensor(word_emb))
        self.rnn = ImprovedRnn(nn.RNN, input_size=gru_in, hidden_size=gru_out,
                               batch_first=True, bidirectional=True)

    def forward(self, seq1, seq1_lengths):
        seq1_emb = self.encode(seq1)
        rnn1, h_n = self.rnn(seq1_emb, seq1_lengths)
        return rnn1, h_n
